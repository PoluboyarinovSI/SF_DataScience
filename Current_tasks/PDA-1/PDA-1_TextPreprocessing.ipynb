{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.3"},"colab":{"name":"TextPreprocessing.ipynb","provenance":[]}},"cells":[{"cell_type":"markdown","metadata":{"id":"bXTFLI4NF-LR"},"source":["## Этапы (простой) обработки текста\n","\n","<img src=\"images/textm.png\">"]},{"cell_type":"markdown","metadata":{"id":"rgf1SvTRF-LS"},"source":["\n","## Декодирование\n","\n","\n","**Def.**  \n","перевод последовательности байт в последовательность символов\n","\n","* Распаковка  \n","*plain/.zip/.gz/...*\n","* Кодировка  \n","*ASCII/utf-8/Windows-1251/...*\n","* Формат  \n","*csv/xml/json/doc...*\n","\n","Кроме того: что такое документ?\n","\n"]},{"cell_type":"markdown","metadata":{"id":"I3xU0-PjF-LS"},"source":["## Разбиение на токены\n","**Def.**  \n","разбиение последовательности символов на части (токены), возможно, исключая из рассмотрения некоторые символы  \n","Наивный подход: разделить строку пробелами и выкинуть знаки препинания  \n","\n","\n","*Трисия любила Нью-Йорк, поскольку любовь к Нью-Йорку могла положительно повлиять на ее карьеру.*  \n","\n","\n","**Проблемы:**  \n","* example@example.com, 127.0.0.1\n","* С++, C#\n","* York University vs New York University\n","* Зависимость от языка (“Lebensversicherungsgesellschaftsangestellter”, “l’amour”)\n","Альтернатива: n-граммы"]},{"cell_type":"code","metadata":{"id":"BTJ5nrYYF-LT","executionInfo":{"status":"ok","timestamp":1631048074715,"user_tz":-180,"elapsed":1719,"user":{"displayName":"Михаил Баранов","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjVV46Gpmty8si64xPQTsh_aZMsXJnF8ADGg9xOTg=s64","userId":"05195714544608108975"}},"outputId":"7d3bd7ab-d9ec-4f33-98df-2d1a4688b602","colab":{"base_uri":"https://localhost:8080/"}},"source":["import nltk\n","nltk.download('stopwords')"],"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/stopwords.zip.\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":1}]},{"cell_type":"code","metadata":{"id":"Olg2Ah-SF-LV","executionInfo":{"status":"ok","timestamp":1631048074720,"user_tz":-180,"elapsed":50,"user":{"displayName":"Михаил Баранов","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjVV46Gpmty8si64xPQTsh_aZMsXJnF8ADGg9xOTg=s64","userId":"05195714544608108975"}},"outputId":"33cd3cf8-dfdf-4f0f-822b-a322096f35b2","colab":{"base_uri":"https://localhost:8080/"}},"source":["from nltk.tokenize import RegexpTokenizer\n","\n","\n","s = \"Трисия любила Нью-Йорк, поскольку любовь к Нью-Йорку могла положительно повлиять на ее карьеру.\"\n","\n","tokenizer = RegexpTokenizer(\"\\w+|[^\\w\\s]+\")\n","for t in tokenizer.tokenize(s): \n","    print(t)"],"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Трисия\n","любила\n","Нью\n","-\n","Йорк\n",",\n","поскольку\n","любовь\n","к\n","Нью\n","-\n","Йорку\n","могла\n","положительно\n","повлиять\n","на\n","ее\n","карьеру\n",".\n"]}]},{"cell_type":"markdown","metadata":{"id":"buEIu-_CF-LV"},"source":["## Стоп-слова\n","**Def.**  \n","Наиболее частые слова в языке, не содержащие никакой информации о содержании текста\n","\n"]},{"cell_type":"code","metadata":{"id":"uZL-XwB9F-LW"},"source":["from nltk.corpus import stopwords\n","\n","\n","print(\" \".join(stopwords.words(\"russian\")[1:20]))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"aXzzCadWF-LW"},"source":["Проблема: “To be or not to be\""]},{"cell_type":"markdown","metadata":{"id":"M2vMM3qnF-LW"},"source":["## Нормализация\n","**Def.**  \n","Приведение токенов к единому виду для того, чтобы избавиться от поверхностной разницы в написании  \n","\n","Подходы  \n","* сформулировать набор правил, по которым преобразуется токен  \n","Нью-Йорк → нью-йорк → ньюйорк → ньюиорк\n","* явно хранить связи между токенами (WordNet – Princeton)  \n","машина → автомобиль, Windows 6→ window"]},{"cell_type":"code","metadata":{"id":"INpfX6wqF-LX"},"source":["s = \"Нью-Йорк\"\n","s1 = s.lower()\n","print(s1)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Wa4NDjrIF-LX"},"source":["import re\n","s2 = re.sub(r\"\\W\", \"\", s1, flags=re.U)\n","print(s2)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ESECQgj2F-LY"},"source":["s3 = re.sub(r\"й\", u\"и\", s2, flags=re.U)\n","print(s3)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"mxgUV4UbF-LY"},"source":["## Стемминг и Лемматизация\n","**Def.**  \n","Приведение грамматических форм слова и однокоренных слов к единой основе (lemma):\n","* Stemming – с помощью простых эвристических правил\n","  * Porter (Cambridge – 1980)\n","        5 этапов, на каждом применяется набор правил, таких как\n","            sses → ss (caresses → caress)\n","            ies → i (ponies → poni)\n","\n","  * Lovins (1968)\n","  * Paice (1990)\n","  * другие\n","* Lemmatization – с использованием словарей и морфологического анализа\n"]},{"cell_type":"markdown","metadata":{"id":"RgDqrYkdF-LZ"},"source":["## Стемминг"]},{"cell_type":"code","metadata":{"id":"fgpAl1MJF-LZ"},"source":["from nltk.stem.snowball import PorterStemmer\n","from nltk.stem.snowball import RussianStemmer\n","\n","\n","s = PorterStemmer()\n","print(s.stem(\"Tokenization\"))\n","print(s.stem(\"stemming\"))\n","\n","r = RussianStemmer()\n","print(r.stem(\"Авиация\"))\n","print(r.stem(\"национальный\"))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xfUJXVE4F-LZ"},"source":["**Наблюдение**  \n","для сложных языков лучше подходит лемматизация"]},{"cell_type":"markdown","metadata":{"id":"6nwjaX0zF-La"},"source":["## Лемматизация"]},{"cell_type":"code","metadata":{"id":"Bq8bFg_DF-La"},"source":["# !pip install pymorphy2"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"qSekO34BF-La"},"source":["import pymorphy2\n","\n","\n","morph = pymorphy2.MorphAnalyzer()\n","print(morph.parse(\"думающему\")[0].normal_form)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xc6Z0UY9F-La"},"source":["## Heaps' law\n","Эмпирическая закономерность в лингвистике, описывающая распределение числа уникальных слов в документе (или наборе документов) как функцию от его длины.\n","\n","$$\n","M = k T^\\beta, \\;M \\text{ -- размер словаря}, \\; T \\text{ -- количество слов в корпусе}\n","$$\n","$$\n","30 \\leq k \\leq 100, \\; b \\approx 0.5\n","$$\n","\n","<img src=\"images/dim.png\">\n","<img src=\"images/heaps.png\">"]},{"cell_type":"markdown","metadata":{"id":"FzDJlw5OF-Lb"},"source":["## Представление документов\n","**Boolean Model.** Присутствие или отсутствие слова в документе  \n","**Bag of Words.** Порядок токенов не важен  \n","\n","*Погода была ужасная, принцесса была прекрасная.\n","Или все было наоборот?*\n","\n","Координаты\n","* Мультиномиальные: количество токенов в документе\n","* Числовые: взвешенное количество токенов в документе"]},{"cell_type":"markdown","metadata":{"id":"7TlHOiiCF-Lb"},"source":["## Zipf's law\n","Эмпирическая закономерность распределения частоты слов естественного языка\n","\n","$t_1, \\ldots, t_N$ - токены, отранжированные по убыванию частоты\n","   \t\n","$f_1, \\dots, f_N$ - соответствующие частоты\n","\n","**Закон Ципфа**\n","\t$$\n","\tf_i = \\frac{c}{i^k}\n","\t$$\t\n","\t\n","\tЧто еще? Посещаемость сайтов, количество друзей, население городов...\n","<img src=\"images/zipf.png\">\n"]}]}