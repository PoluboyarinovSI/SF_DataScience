{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## В этой тетрадке посмотрим, как настроить предобученную библиотеку BERT для классификации текста"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Возможно вы слышали о существовании модели BERT. Это нейронная сеть от Google, показавшая отличные результаты на целом ряде задач. С помощью BERT можно создавать программы с ИИ для обработки естественного языка: отвечать на вопросы, заданные в произвольной форме, создавать чат-ботов, автоматические переводчики, анализировать текст и так далее.\n",
    "\n",
    "**Идея в основе BERT лежит очень простая:** давайте на вход нейросети будем подавать фразы, в которых 15% слов заменим на [MASK], и обучим нейронную сеть предсказывать эти закрытые маской слова.\n",
    "\n",
    "Например, если подаем на вход нейросети фразу **\"Я пришел в [MASK] и купил [MASK]\"**, она должна на выходе показать слова **\"магазин\"** и **\"молоко\"**. Это упрощенный пример с официальной страницы BERT, на более длинных предложениях разброс возможных вариантов становится меньше, а ответ нейросети однозначнее.\n",
    "\n",
    "А для того, чтобы нейросеть научилась понимать соотношения между разными предложениями, дополнительно обучим ее предсказывать, является ли вторая фраза логичным продолжением первой. Или это какая-то случайная фраза, не имеющая никакого отношения к первой.\n",
    "\n",
    "Так, для двух предложений: **\"Я пошел в магазин.\"** и **\"И купил там молоко.\"**, нейросеть должна ответить, что это логично. А если вторая фраза будет **\"Карась небо Плутон\"**, то должна ответить, что это предложение никак не связано с первым. Ниже мы поиграемся с обоими этими режимами работы BERT.\n",
    "\n",
    "**Обучив таким образом нейронную сеть на корпусе текстов из Wikipedia и сборнике книг BookCorpus в течении 4 дней на 16 TPU, получили BERT.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задача, которую будем решать с помощью BERT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Мы настроим модель BERT для решения задачи классификации текста. Делать это будем с помощью библиотеки Transformers. \n",
    "\n",
    "Установим ее."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, http://nexus.mt.ru:8081/repository/pypi-hosted/simple\n",
      "Collecting transformers\n",
      "  Downloading transformers-4.14.1-py3-none-any.whl (3.4 MB)\n",
      "\u001b[K     |████████████████████████████████| 3.4 MB 1.2 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /Users/y.mochalova/opt/anaconda3/lib/python3.8/site-packages (from transformers) (1.19.5)\n",
      "Requirement already satisfied: tqdm>=4.27 in /Users/y.mochalova/opt/anaconda3/lib/python3.8/site-packages (from transformers) (4.50.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/y.mochalova/opt/anaconda3/lib/python3.8/site-packages (from transformers) (21.0)\n",
      "Collecting huggingface-hub<1.0,>=0.1.0\n",
      "  Downloading huggingface_hub-0.2.1-py3-none-any.whl (61 kB)\n",
      "\u001b[K     |████████████████████████████████| 61 kB 1.2 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting sacremoses\n",
      "  Downloading sacremoses-0.0.46-py3-none-any.whl (895 kB)\n",
      "\u001b[K     |████████████████████████████████| 895 kB 5.9 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: filelock in /Users/y.mochalova/opt/anaconda3/lib/python3.8/site-packages (from transformers) (3.0.12)\n",
      "Collecting tokenizers<0.11,>=0.10.1\n",
      "  Downloading tokenizers-0.10.3-cp38-cp38-macosx_10_11_x86_64.whl (2.2 MB)\n",
      "\u001b[K     |████████████████████████████████| 2.2 MB 9.5 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: pyyaml>=5.1 in /Users/y.mochalova/opt/anaconda3/lib/python3.8/site-packages (from transformers) (5.4.1)\n",
      "Requirement already satisfied: requests in /Users/y.mochalova/opt/anaconda3/lib/python3.8/site-packages (from transformers) (2.26.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/y.mochalova/opt/anaconda3/lib/python3.8/site-packages (from transformers) (2021.9.24)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /Users/y.mochalova/opt/anaconda3/lib/python3.8/site-packages (from packaging>=20.0->transformers) (2.4.7)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/y.mochalova/opt/anaconda3/lib/python3.8/site-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (3.7.4.3)\n",
      "Requirement already satisfied: joblib in /Users/y.mochalova/opt/anaconda3/lib/python3.8/site-packages (from sacremoses->transformers) (0.17.0)\n",
      "Requirement already satisfied: six in /Users/y.mochalova/opt/anaconda3/lib/python3.8/site-packages (from sacremoses->transformers) (1.16.0)\n",
      "Requirement already satisfied: click in /Users/y.mochalova/opt/anaconda3/lib/python3.8/site-packages (from sacremoses->transformers) (7.1.2)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/y.mochalova/opt/anaconda3/lib/python3.8/site-packages (from requests->transformers) (1.26.7)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0; python_version >= \"3\" in /Users/y.mochalova/opt/anaconda3/lib/python3.8/site-packages (from requests->transformers) (2.0.7)\n",
      "Requirement already satisfied: idna<4,>=2.5; python_version >= \"3\" in /Users/y.mochalova/opt/anaconda3/lib/python3.8/site-packages (from requests->transformers) (2.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/y.mochalova/opt/anaconda3/lib/python3.8/site-packages (from requests->transformers) (2020.12.5)\n",
      "Installing collected packages: huggingface-hub, sacremoses, tokenizers, transformers\n",
      "Successfully installed huggingface-hub-0.2.1 sacremoses-0.0.46 tokenizers-0.10.3 transformers-4.14.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "#pip install transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Импортируем необходимые библиотеки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Пользователь\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\scipy\\__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "import transformers\n",
    "from transformers import AutoModel, BertTokenizerFast\n",
    "\n",
    "device = torch.device(\"cuda\") \n",
    "# Если эта операция не достуана - пропустите ее.\n",
    "# Tсли в процессе выполнения возникаеn ошибка \"Torch not compiled with CUDA enabled\" - \n",
    "# также, выполняйте задание без использования .to(drvice)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Загрузим набор данных.\n",
    "\n",
    "Если вы работаете в Colab, то сначала вам нужно будет загрузить набор данных о спаме в среду выполнения Colab. Затем прочитайте его как обычно, с использованием pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label                                               text\n",
       "0      0  Go until jurong point, crazy.. Available only ...\n",
       "1      0                      Ok lar... Joking wif u oni...\n",
       "2      1  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "3      0  U dun say so early hor... U c already then say...\n",
       "4      0  Nah I don't think he goes to usf, he lives aro..."
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"C:\\SkillFactory\\SF_DataScience\\Current_tasks\\data\\spam.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Набор данных состоит из двух столбцов - «label» и «текст». Столбец «текст» содержит текст сообщения, а «label» - это двоичная переменная, где 1 означает спам, а 0 означает, что сообщение не является спамом.\n",
    "\n",
    "Теперь мы разделим этот набор данных на три набора - обучающий, проверочный и тестовый."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split train dataset into train, validation and test sets\n",
    "train_text, temp_text, train_labels, temp_labels = train_test_split(df['text'], df['label'], \n",
    "                                                        random_state=42, \n",
    "                                                        test_size=0.3, \n",
    "                                                        stratify=df['label'])\n",
    "val_text, test_text, val_labels, test_labels = train_test_split(temp_text, temp_labels, \n",
    "                                                    random_state=42, \n",
    "                                                    test_size=0.5, \n",
    "                                                    stratify=temp_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Мы настроим модель, используя train и validation set, и сделаем прогнозы для test.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь импортируем модели BERT и токенизатора BERT\n",
    "Импортируем базовую модель BERT, которая имеет 110 миллионов параметров. Существует еще более крупная модель BERT, называемая BERT-large, которая имеет 345 миллионов параметров."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.10188126564025879,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "config.json",
       "rate": null,
       "total": 570,
       "unit": "B",
       "unit_divisor": 1000,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "268169d688b3497ca049716ed20378ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Пользователь\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\huggingface_hub\\file_download.py:147: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Пользователь\\.cache\\huggingface\\hub. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.14766716957092285,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "model.safetensors",
       "rate": null,
       "total": 440449768,
       "unit": "B",
       "unit_divisor": 1000,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "99a5b4288519449a9d5a50ad8d1a5be8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.08904552459716797,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "tokenizer_config.json",
       "rate": null,
       "total": 28,
       "unit": "B",
       "unit_divisor": 1000,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f92b88ab2ce4cdd976265063b081b43",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/28.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.08473706245422363,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "vocab.txt",
       "rate": null,
       "total": 231508,
       "unit": "B",
       "unit_divisor": 1000,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "12d07c8446bb41e5bb736784552b870d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.12401223182678223,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "tokenizer.json",
       "rate": null,
       "total": 466062,
       "unit": "B",
       "unit_divisor": 1000,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e61a57fb337453d862d0414e9b00e8f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# import BERT-base pretrained model\n",
    "bert = AutoModel.from_pretrained('bert-base-uncased')\n",
    "# Load the BERT tokenizer\n",
    "tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Давайте посмотрим, как работает токенизатор BERT. Попробуем закодировать пару предложений с помощью токенизатора."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [[101, 2023, 2003, 1037, 14324, 2944, 14924, 4818, 102, 0], [101, 2057, 2097, 2986, 1011, 8694, 1037, 14324, 2944, 102]], 'token_type_ids': [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]}\n"
     ]
    }
   ],
   "source": [
    "# sample data\n",
    "text = [\"this is a bert model tutorial\", \"we will fine-tune a bert model\"]\n",
    "# encode text\n",
    "sent_id = tokenizer.batch_encode_plus(text, padding=True)\n",
    "# output\n",
    "print(sent_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как видите, на выходе получается словарь из двух элементов.\n",
    "\n",
    "«Input_ids» содержит целочисленные последовательности входных предложений. Целые числа 101 и 102 - особые токены. Мы добавляем их к обеим последовательностям, а 0 представляет собой маркер заполнения.\n",
    "«attention_mask» содержит единицы и нули. Она сообщает модели, что нужно обращать внимание на токены, соответствующие значению маски, равному 1, и игнорировать остальные."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Токинизируем предложения\n",
    "Поскольку сообщения (текст) в наборе данных имеют разную длину, мы будем использовать заполнение, чтобы все сообщения имели одинаковую длину. Мы можем использовать максимальную длину последовательности для заполнения сообщений. Однако мы также можем посмотреть на распределение длин последовательностей в обучающей выборке, чтобы найти правильную длину заполнения."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# get length of all the messages in the train set\n",
    "seq_len = [len(i.split()) for i in train_text]\n",
    "pd.Series(seq_len).hist(bins = 30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Видно, что большинство сообщений имеют длину 25 слов и меньше. В то время как максимальная длина составляет 175. Итак, если мы выберем 175 в качестве длины заполнения, тогда все входные последовательности будут иметь длину 175, и большинство токенов в этих последовательностях будут токенами заполнения (нулями), которые не помогут модели изучить что-либо полезное и кроме того, это замедлит обучение.\n",
    "\n",
    "Поэтому мы установим 25 как длину сообщения, и сообщения бОльшим размером будем укорачивать."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Пользователь\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2618: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# tokenize and encode sequences in the training set\n",
    "tokens_train = tokenizer.batch_encode_plus(\n",
    "train_text.tolist(),\n",
    "max_length = 25,\n",
    "pad_to_max_length=True,\n",
    "truncation=True\n",
    ")\n",
    "# tokenize and encode sequences in the validation set\n",
    "tokens_val = tokenizer.batch_encode_plus(\n",
    "val_text.tolist(),\n",
    "max_length = 25,\n",
    "pad_to_max_length=True,\n",
    "truncation=True\n",
    ")\n",
    "# tokenize and encode sequences in the test set\n",
    "tokens_test = tokenizer.batch_encode_plus(\n",
    "test_text.tolist(),\n",
    "max_length = 25,\n",
    "pad_to_max_length=True,\n",
    "truncation=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Итак, теперь мы преобразовали сообщения в train, valid и test наборах в целочисленные последовательности длиной 25 токенов каждая.\n",
    "\n",
    "Затем мы преобразуем целочисленные последовательности в тензоры. (Тензоры - это альтернатива векторам numpy, но для нейронных сетей, испульзуюся для вычислений в PyTorch.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## convert lists to tensors\n",
    "train_seq = torch.tensor(tokens_train['input_ids'])\n",
    "train_mask = torch.tensor(tokens_train['attention_mask'])\n",
    "train_y = torch.tensor(train_labels.tolist())\n",
    "val_seq = torch.tensor(tokens_val['input_ids'])\n",
    "val_mask = torch.tensor(tokens_val['attention_mask'])\n",
    "val_y = torch.tensor(val_labels.tolist())\n",
    "test_seq = torch.tensor(tokens_test['input_ids'])\n",
    "test_mask = torch.tensor(tokens_test['attention_mask'])\n",
    "test_y = torch.tensor(test_labels.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь мы создадим dataloaders. Dataloaders будут передавать батчи (батч - это группа объектов) train и valid наборов в качестве входных данных для модели на этапе обучения."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "#define a batch size\n",
    "batch_size = 32\n",
    "# wrap tensors\n",
    "train_data = TensorDataset(train_seq, train_mask, train_y)\n",
    "# sampler for sampling the data during training\n",
    "train_sampler = RandomSampler(train_data)\n",
    "# dataLoader for train set\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
    "# wrap tensors\n",
    "val_data = TensorDataset(val_seq, val_mask, val_y)\n",
    "# sampler for sampling the data during training\n",
    "val_sampler = SequentialSampler(val_data)\n",
    "# dataLoader for validation set\n",
    "val_dataloader = DataLoader(val_data, sampler = val_sampler, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Определим архитектуру модели\n",
    "\n",
    "Установим параметру requires_grad значение false. Это предотвратит обновление исходных весов предобученной модели во время донастройки. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# freeze all the parameters\n",
    "for param in bert.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Двигаемся дальше, давайте теперь определим нашу архитектуру модели."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERT_Arch(nn.Module):\n",
    "    def __init__(self, bert):\n",
    "        super(BERT_Arch, self).__init__()\n",
    "        self.bert = bert \n",
    "        # dropout layer\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        # relu activation function\n",
    "        self.relu =  nn.ReLU()\n",
    "        # dense layer 1\n",
    "        self.fc1 = nn.Linear(768,512)\n",
    "        # dense layer 2 (Output layer)\n",
    "        self.fc2 = nn.Linear(512,2)\n",
    "        #softmax activation function\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "        \n",
    "    #define the forward pass\n",
    "    def forward(self, sent_id, mask):\n",
    "        #pass the inputs to the model  \n",
    "        new_bert = self.bert(sent_id, attention_mask=mask)\n",
    "        _, cls_hs = new_bert[0],new_bert[1]\n",
    "        x = self.fc1(cls_hs)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        # output layer\n",
    "        x = self.fc2(x)\n",
    "        # apply softmax activation\n",
    "        x = self.softmax(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pass the pre-trained BERT to our define architecture\n",
    "model = BERT_Arch(bert)\n",
    "# push the model to GPU\n",
    "#model = model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Мы будем использовать AdamW в качестве нашего оптимизатора. Это улучшенная версия оптимизатора Adam. Чтобы узнать больше об этом, ознакомьтесь с этой статьей."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Пользователь\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\optimization.py:429: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# optimizer from hugging face transformers\n",
    "from transformers import AdamW\n",
    "# define the optimizer\n",
    "optimizer = AdamW(model.parameters(),\n",
    "lr = 1e-5)          # learning rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В нашем наборе данных наблюдается дисбаланс классов. Большинство наблюдений не являются спамом. Итак, мы сначала вычислим веса классов для меток в наборе поездов, а затем передадим эти веса в функцию потерь, чтобы она позаботилась о дисбалансе классов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class Weights: [0.57743559 3.72848948]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "#compute the class weights\n",
    "class_weights = compute_class_weight(class_weight='balanced', classes=np.unique(train_labels), y=train_labels)\n",
    "print(\"Class Weights:\", class_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting list of class weights to a tensor\n",
    "weights= torch.tensor(class_weights,dtype=torch.float)\n",
    "# push to GPU\n",
    "#weights = weights.to(device)\n",
    "# define the loss function\n",
    "cross_entropy  = nn.NLLLoss(weight=weights) \n",
    "# number of training epochs\n",
    "epochs = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Точная настройка BERT\n",
    "Итак, до сих пор мы определили архитектуру модели, мы указали оптимизатор и функцию потерь, и наши загрузчики данных также готовы. Теперь нам нужно определить пару функций для обучения (точной настройки) и оценки модели соответственно."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to train the model\n",
    "def train():\n",
    "    model.train()\n",
    "    total_loss, total_accuracy = 0, 0\n",
    "    # empty list to save model predictions\n",
    "    total_preds=[]\n",
    "    # iterate over batches\n",
    "    for step,batch in enumerate(train_dataloader):\n",
    "        # progress update after every 50 batches.\n",
    "        if step % 50 == 0 and not step == 0:\n",
    "            print('  Batch {:>5,}  of  {:>5,}.'.format(step, len(train_dataloader)))\n",
    "        # push the batch to gpu\n",
    "        #batch = [r.to(device) for r in batch]\n",
    "        sent_id, mask, labels = batch\n",
    "        # clear previously calculated gradients \n",
    "        model.zero_grad()        \n",
    "        # get model predictions for the current batch\n",
    "        preds = model(sent_id, mask)\n",
    "        # compute the loss between actual and predicted values\n",
    "        loss = cross_entropy(preds, labels)\n",
    "        # add on to the total loss\n",
    "        total_loss = total_loss + loss.item()\n",
    "        # backward pass to calculate the gradients\n",
    "        loss.backward()\n",
    "        # clip the the gradients to 1.0. It helps in preventing the exploding gradient problem\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        # update parameters\n",
    "        optimizer.step()\n",
    "        # model predictions are stored on GPU. So, push it to CPU\n",
    "        preds=preds.detach().cpu().numpy()\n",
    "        # append the model predictions\n",
    "        total_preds.append(preds)\n",
    "    # compute the training loss of the epoch\n",
    "    avg_loss = total_loss / len(train_dataloader)\n",
    "    # predictions are in the form of (no. of batches, size of batch, no. of classes).\n",
    "    # reshape the predictions in form of (number of samples, no. of classes)\n",
    "    total_preds  = np.concatenate(total_preds, axis=0)\n",
    "    #returns the loss and predictions\n",
    "    return avg_loss, total_preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Мы будем использовать следующую функцию для оценки модели. Он будет использовать данные набора проверки."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for evaluating the model\n",
    "def evaluate():\n",
    "    print(\"\\nEvaluating...\")\n",
    "    # deactivate dropout layers\n",
    "    model.eval()\n",
    "    total_loss, total_accuracy = 0, 0\n",
    "    # empty list to save the model predictions\n",
    "    total_preds = []\n",
    "    # iterate over batches\n",
    "    for step,batch in enumerate(val_dataloader):\n",
    "        # Progress update every 50 batches.\n",
    "        if step % 50 == 0 and not step == 0:\n",
    "            # Calculate elapsed time in minutes.\n",
    "            elapsed = format_time(time.time() - t0)\n",
    "            # Report progress.\n",
    "            print('  Batch {:>5,}  of  {:>5,}.'.format(step, len(val_dataloader)))\n",
    "        # push the batch to gpu\n",
    "        #batch = [t.to(device) for t in batch]\n",
    "        sent_id, mask, labels = batch\n",
    "        # deactivate autograd\n",
    "        with torch.no_grad():\n",
    "            # model predictions\n",
    "            preds = model(sent_id, mask)\n",
    "            # compute the validation loss between actual and predicted values\n",
    "            loss = cross_entropy(preds,labels)\n",
    "            total_loss = total_loss + loss.item()\n",
    "            preds = preds.detach().cpu().numpy()\n",
    "            total_preds.append(preds)\n",
    "    # compute the validation loss of the epoch\n",
    "    avg_loss = total_loss / len(val_dataloader) \n",
    "    # reshape the predictions in form of (number of samples, no. of classes)\n",
    "    total_preds  = np.concatenate(total_preds, axis=0)\n",
    "    return avg_loss, total_preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь наконец приступим к настройке модели."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Epoch 1 / 10\n",
      "  Batch    50  of    122.\n",
      "  Batch   100  of    122.\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 0.675\n",
      "Validation Loss: 0.655\n",
      "\n",
      " Epoch 2 / 10\n",
      "  Batch    50  of    122.\n",
      "  Batch   100  of    122.\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 0.645\n",
      "Validation Loss: 0.627\n",
      "\n",
      " Epoch 3 / 10\n",
      "  Batch    50  of    122.\n",
      "  Batch   100  of    122.\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 0.617\n",
      "Validation Loss: 0.601\n",
      "\n",
      " Epoch 4 / 10\n",
      "  Batch    50  of    122.\n",
      "  Batch   100  of    122.\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 0.590\n",
      "Validation Loss: 0.574\n",
      "\n",
      " Epoch 5 / 10\n",
      "  Batch    50  of    122.\n",
      "  Batch   100  of    122.\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 0.576\n",
      "Validation Loss: 0.553\n",
      "\n",
      " Epoch 6 / 10\n",
      "  Batch    50  of    122.\n",
      "  Batch   100  of    122.\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 0.546\n",
      "Validation Loss: 0.531\n",
      "\n",
      " Epoch 7 / 10\n",
      "  Batch    50  of    122.\n",
      "  Batch   100  of    122.\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 0.526\n",
      "Validation Loss: 0.512\n",
      "\n",
      " Epoch 8 / 10\n",
      "  Batch    50  of    122.\n",
      "  Batch   100  of    122.\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 0.506\n",
      "Validation Loss: 0.491\n",
      "\n",
      " Epoch 9 / 10\n",
      "  Batch    50  of    122.\n",
      "  Batch   100  of    122.\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 0.492\n",
      "Validation Loss: 0.473\n",
      "\n",
      " Epoch 10 / 10\n",
      "  Batch    50  of    122.\n",
      "  Batch   100  of    122.\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 0.476\n",
      "Validation Loss: 0.457\n"
     ]
    }
   ],
   "source": [
    "# set initial loss to infinite\n",
    "best_valid_loss = float('inf')\n",
    "# empty lists to store training and validation loss of each epoch\n",
    "train_losses=[]\n",
    "valid_losses=[]\n",
    "#for each epoch\n",
    "for epoch in range(epochs):\n",
    "    print('\\n Epoch {:} / {:}'.format(epoch + 1, epochs))\n",
    "    #train model\n",
    "    train_loss, _ = train()\n",
    "    #evaluate model\n",
    "    valid_loss, _ = evaluate()\n",
    "    #save the best model\n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), 'saved_weights.pt')\n",
    "    # append training and validation loss\n",
    "    train_losses.append(train_loss)\n",
    "    valid_losses.append(valid_loss)\n",
    "    print(f'\\nTraining Loss: {train_loss:.3f}')\n",
    "    print(f'Validation Loss: {valid_loss:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вы можете видеть, что лосс на валидации все еще уменьшается в конце 10-й эпохи. Это значит, что вы можете попробовать большее количество эпох и достичь наилучшего рещультата. Теперь давайте посмотрим, насколько хорошо модель работает на тестовом наборе данных.\n",
    "\n",
    "\n",
    "## Сделаем предсказания\n",
    "Для прогноза загрузим лучшие веса модели, которые были сохранены в процессе обучения."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#load weights of best model\n",
    "path = 'saved_weights.pt'\n",
    "model.load_state_dict(torch.load(path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как только веса загружены, мы можем использовать точно настроенную модель."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get predictions for test data\n",
    "with torch.no_grad():\n",
    "    preds = model(test_seq, test_mask)#(test_seq.to(device), test_mask.to(device))\n",
    "    preds = preds.detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Давайте посмотрим на результаты"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.86      0.91       724\n",
      "           1       0.48      0.84      0.61       112\n",
      "\n",
      "    accuracy                           0.86       836\n",
      "   macro avg       0.73      0.85      0.76       836\n",
      "weighted avg       0.91      0.86      0.87       836\n",
      "\n"
     ]
    }
   ],
   "source": [
    "preds = np.argmax(preds, axis = 1)\n",
    "print(classification_report(test_y, preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "И recall, и precision для класса 0 довольно высоки, что означает, что модель довольно хорошо предсказывает этот класс. Однако нашей целью было обнаружение спам-сообщений, поэтому неверная классификация образцов класса 1 (спам) является более серьезной проблемой, чем неправильная классификация образцов класса 0. Если вы посмотрите на recall для класса 1, он равен 0.75, что означает, что модель смогла правильно классифицировать 75% спам-сообщений."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Итог\n",
    "\n",
    "Как уже сказано выше, вы можете добиться лучшей точности, обучив модель на большем количестве эпох. Но даже сейчас мы получили неплохой результат для работы с текстом. \n",
    "\n",
    "Обязательно попробуйте модель BERT, если перед вами будут стоять задачи по обработке тек"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
